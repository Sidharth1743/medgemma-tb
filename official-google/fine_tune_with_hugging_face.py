# -*- coding: utf-8 -*-
"""fine_tune_with_hugging_face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/google-health/medsiglip/blob/main/notebooks/fine_tune_with_hugging_face.ipynb

~~~
Copyright 2025 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
~~~

# Fine-tune MedSigLIP with Hugging Face

<table><tbody><tr>
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/google-health/medsiglip/blob/main/notebooks/fine_tune_with_hugging_face.ipynb">
      <img alt="Google Colab logo" src="https://www.tensorflow.org/images/colab_logo_32px.png" width="32px"><br> Run in Google Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fmedsiglip%2Fmain%2Fnotebooks%2Ffine_tune_with_hugging_face.ipynb">
      <img alt="Google Cloud Colab Enterprise logo" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" width="32px"><br> Run in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/google-health/medsiglip/blob/main/notebooks/fine_tune_with_hugging_face.ipynb">
      <img alt="GitHub logo" src="https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png" width="32px"><br> View on GitHub
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://huggingface.co/collections/google/medsiglip-release-680aade845f90bec6a3f60c4">
      <img alt="Hugging Face logo" src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="32px"><br> View on Hugging Face
    </a>
  </td>
</tr></tbody></table>

This notebook demonstrates fine-tuning MedSigLIP on an image and text dataset for contrastive learning using Hugging Face libraries.

## Setup

To complete this tutorial, you'll need to have a runtime with sufficient resources to fine-tune the MedSigLIP model. **Note:** This guide requires a GPU that has at least 40 GB of memory.

You can run this notebook in Google Colab using an A100 GPU:

1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.
2. Select **Change runtime type**.
3. Under **Hardware accelerator**, select **A100 GPU**.

### Get access to MedSigLIP

Before you get started, make sure that you have access to MedSigLIP models on Hugging Face:

1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).
2. Head over to the [MedSigLIP model page](https://huggingface.co/google/medsiglip-448) and accept the usage conditions.

### Configure your HF token

Generate a Hugging Face `write` access token by going to [settings](https://huggingface.co/settings/tokens). **Note:** Make sure that the token has write access to push the fine-tuned model to Hugging Face Hub.

If you are using Google Colab, add your access token to the Colab Secrets manager to securely store it. If not, proceed to run the cell below to authenticate with Hugging Face.

1. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel. <img src="https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg" alt="The Secrets tab is found on the left panel." width=50%>
2. Create a new secret with the name `HF_TOKEN`.
3. Copy/paste your token key into the Value input box of `HF_TOKEN`.
4. Toggle the button on the left to allow notebook access to the secret.
"""

import os
import sys

if "google.colab" in sys.modules and not os.environ.get("VERTEX_PRODUCT"):
    # Use secret if running in Google Colab
    from google.colab import userdata
    os.environ["HF_TOKEN"] = userdata.get("HF_TOKEN")
else:
    # Store Hugging Face data under `/content` if running in Colab Enterprise
    if os.environ.get("VERTEX_PRODUCT") == "COLAB_ENTERPRISE":
        os.environ["HF_HOME"] = "/content/hf"
    # Authenticate with Hugging Face
    from huggingface_hub import get_token
    if get_token() is None:
        from huggingface_hub import notebook_login
        notebook_login()

"""### Install dependencies"""

! pip install --upgrade --quiet accelerate datasets evaluate tensorboard transformers

"""## Load model from Hugging Face Hub"""

import torch
from transformers import AutoProcessor, AutoModel

model_id = "google/medsiglip-448"

model = AutoModel.from_pretrained(model_id)
processor = AutoProcessor.from_pretrained(model_id)

"""## Prepare fine-tuning dataset

This notebook uses the [NCT-CRC-HE-100K](https://zenodo.org/records/1214456) dataset, containing image patches from histological images of human colorectal cancer (CRC) and normal tissue, to fine-tune MedSigLIP.

**Note:** The full NCT-CRC-HE-100K dataset contains 100K samples. By default this guide only uses a subset with 10,000 samples to keep the training example small, but you can adjust this number if you want to experiment.

**Dataset citation:** Kather, J. N., Halama, N., & Marx, A. (2018). 100,000 histological images of human colorectal cancer and healthy tissue (v0.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1214456

Download the dataset. This step may take around 15 minutes to complete.
"""

! wget -nc -q "https://zenodo.org/records/1214456/files/NCT-CRC-HE-100K.zip"
! unzip -q NCT-CRC-HE-100K.zip

"""Load the data using the Hugging Face `datasets` library. Then, create train and validation splits."""

from datasets import load_dataset

train_size = 9000  # @param {type: "number"}
validation_size = 1000  # @param {type: "number"}

data = load_dataset("./NCT-CRC-HE-100K", split="train")
data = data.train_test_split(
    train_size=train_size,
    test_size=validation_size,
    shuffle=True,
    seed=42,
)
# Use the test split as the validation set
data["validation"] = data.pop("test")

# Display dataset details
data

"""Inspect a sample data point, which contains:

* `image`: image patch as a `PIL` image object
* `label`: integer class label corresponding to tissue type
"""

data["train"][0]["image"]

data["train"][0]["label"]

"""Preprocess the input images, including resizing and rescaling, and tokenize captions. The text captions are directly the tissue class names."""

from torchvision.transforms import Compose, Resize, ToTensor, Normalize, InterpolationMode

TISSUE_CLASSES = [
    "adipose",
    "background",
    "debris",
    "lymphocytes",
    "mucus",
    "smooth muscle",
    "normal colon mucosa",
    "cancer-associated stroma",
    "colorectal adenocarcinoma epithelium"
]

size = processor.image_processor.size["height"]
mean = processor.image_processor.image_mean
std = processor.image_processor.image_std

_transform = Compose([
    Resize((size, size), interpolation=InterpolationMode.BILINEAR),
    ToTensor(),
    Normalize(mean=mean, std=std),
])


def preprocess(examples):
    pixel_values = [_transform(image.convert("RGB")) for image in examples["image"]]
    captions = [TISSUE_CLASSES[label] for label in examples["label"]]
    inputs = processor.tokenizer(
        captions,
        max_length=64,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
    )
    inputs["pixel_values"] = pixel_values
    return inputs


data = data.map(preprocess, batched=True, remove_columns=["image", "label"])

"""## Fine-tune the model

Contrastive image-text learning is a method that trains models to learn the relationship between images and text, such that the representations of matching image-text pairs are brought closer together in a shared embedding space while non-matching pairs are pushed further apart.

This notebook demonstrates contrastive fine-tuning of MedSigLIP, where the vision and text encoders are jointly trained on image and text data, using the `Trainer` from the Hugging Face `Transformers` library.

Define a data collator to prepare batches of training examples.
"""

import torch


def collate_fn(examples):
    pixel_values = torch.tensor([example["pixel_values"] for example in examples])
    input_ids = torch.tensor([example["input_ids"] for example in examples])
    attention_mask = torch.tensor([example["attention_mask"] for example in examples])
    return {
        "pixel_values": pixel_values,
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "return_loss": True,
    }

"""Configure training parameters in [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."""

from transformers import TrainingArguments

num_train_epochs = 2  # @param {type: "number"}
learning_rate = 1e-4  # @param {type: "number"}

training_args = TrainingArguments(
    output_dir="medsiglip-448-ft-crc100k",  # Directory and Hub repository id to save the model to
    num_train_epochs=num_train_epochs,      # Number of training epochs
    per_device_train_batch_size=8,          # Batch size per device during training
    per_device_eval_batch_size=8,           # Batch size per device during evaluation
    gradient_accumulation_steps=8,          # Number of steps before performing a backward/update pass
    logging_steps=50,                       # Number of steps between logs
    save_strategy="epoch",                  # Save checkpoint every epoch
    eval_strategy="steps",                  # Evaluate every `eval_steps`
    eval_steps=50,                          # Number of steps between evaluations
    learning_rate=learning_rate,            # Learning rate
    weight_decay=0.01,                      # Weight decay to apply
    warmup_steps=5,                         # Number of steps for linear warmup from 0 to learning rate
    lr_scheduler_type="cosine",             # Use cosine learning rate scheduler
    push_to_hub=True,                       # Push model to Hub
    report_to="tensorboard",                # Report metrics to tensorboard
)

"""Construct a [`Trainer`](https://huggingface.co/docs/transformers/trainer) using the previously defined training parameters and data collator."""

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data["train"],
    eval_dataset=data["validation"].shuffle().select(range(200)),  # Use subset of validation set for faster run
    data_collator=collate_fn,
)

"""Launch the fine-tuning process.

**Note:** This may take around 3 hours to run using the default configuration.
"""

trainer.train()

"""Save the final model to Hugging Face Hub."""

trainer.save_model()

"""## Evaluate the fine-tuned model on a classification task

### Prepare test dataset

The [CRC-VAL-HE-7K](https://zenodo.org/records/1214456) dataset contains image patches from patients with colorectal adenocarcinoma and does not overlap with NCT-CRC-HE-100K. It can be used as the test dataset to evaluate the fine-tuned MedSigLIP model for classifying tissue types.

**Note:** The full CRC-VAL-HE-7K dataset contains over 7K samples. By default this guide only uses a subset with 1,000 samples to keep the evaluation example small.

Download and prepare the test dataset.
"""

! wget -nc -q "https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip"
! unzip -q CRC-VAL-HE-7K.zip

from datasets import load_dataset

test_data = load_dataset("./CRC-VAL-HE-7K", split="train")
test_data = test_data.shuffle(seed=42).select(range(1000))
test_batches = test_data.batch(batch_size=64)

"""### Set up for evaluation

Load the accuracy and F1 score metrics to evaluate the model's performance on the classfication task.
"""

import evaluate

accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

# Ground-truth labels
REFERENCES = test_data["label"]


def compute_metrics(predictions: list[int]) -> dict[str, float]:
    metrics = {}
    metrics.update(accuracy_metric.compute(
        predictions=predictions,
        references=REFERENCES,
    ))
    metrics.update(f1_metric.compute(
        predictions=predictions,
        references=REFERENCES,
        average="weighted",
    ))
    return metrics

"""### Compute baseline metrics on the pretrained model

Load the pretrained model.
"""

from transformers import AutoModel

pt_model = AutoModel.from_pretrained(model_id, device_map="auto")

"""Run batch inference on the test dataset."""

import io

from PIL import Image
import torch

pt_predictions = []
for batch in test_batches:
    images = [Image.open(io.BytesIO(image["bytes"])) for image in batch["image"]]
    inputs = processor(text=TISSUE_CLASSES, images=images, padding="max_length", return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = pt_model(**inputs)

    logits_per_image = outputs.logits_per_image
    pt_predictions.extend(logits_per_image.argmax(axis=1).tolist())

"""Compute metrics."""

pt_metrics = compute_metrics(pt_predictions)
print(f"Baseline metrics: {pt_metrics}")

"""### Compute metrics on the fine-tuned model

Load the fine-tuned model.
"""

ft_model = AutoModel.from_pretrained(training_args.output_dir, device_map="auto")

"""Run batch inference on the test dataset."""

ft_predictions = []
for batch in test_batches:
    images = [Image.open(io.BytesIO(image["bytes"])) for image in batch["image"]]
    inputs = processor(text=TISSUE_CLASSES, images=images, padding="max_length", return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = ft_model(**inputs)

    logits_per_image = outputs.logits_per_image
    ft_predictions.extend(logits_per_image.argmax(axis=1).tolist())

"""Compute metrics."""

ft_metrics = compute_metrics(ft_predictions)
print(f"Fine-tuned metrics: {ft_metrics}")

"""# Next steps

Explore the other [notebooks](https://github.com/google-health/medsiglip/blob/main/notebooks) to learn what else you can do with the model.
"""